{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import VideoDataset,VideoLabelDataset\n",
    "\n",
    "import transforms\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "#from torchvision import transforms\n",
    "from PIL import Image, ImageFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "    videos_root = os.path.join(os.getcwd(), 'demo_videos')\n",
    "    annotation_file = os.path.join(videos_root, 'annotations.txt')\n",
    "\n",
    "    preprocess = transforms.Compose([\n",
    "        ImglistToTensor(),  # list of PIL images to (FRAMES x CHANNELS x HEIGHT x WIDTH) tensor\n",
    "        transforms.Resize(299),  # image batch, resize smaller edge to 299\n",
    "        transforms.CenterCrop(299),  # image batch, center crop to square 299x299\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    dataset = VideoFrameDataset(\n",
    "        root_path=videos_root,\n",
    "        annotationfile_path=annotation_file,\n",
    "        num_segments=5,\n",
    "        frames_per_segment=1,\n",
    "        imagefile_template='img_{:05d}.jpg',\n",
    "        transform=preprocess,\n",
    "        test_mode=False\n",
    "    )\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=2,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "classes = [\"ApplyEyeMakeup\",\"Archery\"] #to store class values\n",
    "idx_to_class = {i:j for i, j in enumerate(classes)}\n",
    "class_to_idx = {value:key for key,value in idx_to_class.items()}\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "params = {\n",
    "#    \"model\": \"resnet50\",\n",
    "    #\"device\": \"cuda\",\n",
    "#    \"lr\": 0.001,\n",
    "    \"batch_size\": 64,\n",
    "    \"num_workers\": 4,\n",
    "    \"n_epochs\": 10,\n",
    "    \"image_size\": 256,\n",
    "    \"in_channels\": 3,\n",
    "    \"num_classes\": 5\n",
    "}\n",
    "\n",
    "image_paths = [\"F:/git/pytorch-VideoDataset/demo_videos/ApplyEyeMakeup/frame000001.jpg\",\n",
    "               \"F:/git/pytorch-VideoDataset/demo_videos/ApplyEyeMakeup/frame000002.jpg\",\n",
    "               \"F:/git/pytorch-VideoDataset/demo_videos/ApplyEyeMakeup/frame000003.jpg\",\n",
    "                \"F:/git/pytorch-VideoDataset/demo_videos/Archery/frame000001.jpg\",\n",
    "                \"F:/git/pytorch-VideoDataset/demo_videos/Archery/frame000002.jpg\",\n",
    "                \"F:/git/pytorch-VideoDataset/demo_videos/Archery/frame000003.jpg\",\n",
    "\"F:/git/pytorch-VideoDataset/demo_videos/Archery/frame000004.jpg\"\n",
    "               ]\n",
    "\n",
    "train_transforms = A.Compose(\n",
    "    [\n",
    "        A.SmallestMaxSize(max_size=350),\n",
    "        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=360, p=0.5),\n",
    "        A.RandomCrop(height=params[\"image_size\"], width=params[\"image_size\"]),\n",
    "        A.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15, p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.5),\n",
    "        A.MultiplicativeNoise(multiplier=[0.5,2], per_channel=True, p=0.2),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n",
    "        A.RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_transforms = A.Compose(\n",
    "    [\n",
    "        A.SmallestMaxSize(max_size=350),\n",
    "        A.CenterCrop(height=params[\"image_size\"], width=params[\"image_size\"]),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "class LandmarkDataset(Dataset):\n",
    "    def __init__(self, image_paths, transform=False):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_filepath = self.image_paths[idx]\n",
    "        image = cv2.imread(image_filepath)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        label = image_filepath.split('/')[-2]\n",
    "        print(label)\n",
    "        label = class_to_idx[label]\n",
    "        print(image_filepath)\n",
    "        if self.transform is not None:\n",
    "            print(\"test:\"+str(label))\n",
    "            image = self.transform(image=image)[\"image\"]\n",
    "\n",
    "        return image, label\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = LandmarkDataset(image_paths,train_transforms)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=params[\"batch_size\"], shuffle=True\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}